# Seminar-Research-Object-Identification
This repository will give a detailed information about Object Identification in different software tools such as Core ML, Google Vision and Microsoft Azure

Rathin’s Video Link: https://sheridanc-my.sharepoint.com/personal/choprrat_shernet_sheridancollege_ca/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fchoprrat%5Fshernet%5Fsheridancollege%5Fca%2FDocuments%2FSeminar%5FRathinChopra%5FVideo%2Emp4&parent=%2Fpersonal%2Fchoprrat%5Fshernet%5Fsheridancollege%5Fca%2FDocuments&originalPath=aHR0cHM6Ly9zaGVyaWRhbmMtbXkuc2hhcmVwb2ludC5jb20vOnY6L2cvcGVyc29uYWwvY2hvcHJyYXRfc2hlcm5ldF9zaGVyaWRhbmNvbGxlZ2VfY2EvRVFlc3JxeUV0aDVPaW8wcGs5M0NTejRCQ3pvR3VMSW9STjl0UnVKQVVsc1NnQT9ydGltZT0zMGhGUkpfbDEwZw

 

Deep’s Video Link: https://sheridanc-my.sharepoint.com/personal/patel795_shernet_sheridancollege_ca/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpatel795%5Fshernet%5Fsheridancollege%5Fca%2FDocuments%2FSeminarVideo%2Emp4&parent=%2Fpersonal%2Fpatel795%5Fshernet%5Fsheridancollege%5Fca%2FDocuments&originalPath=aHR0cHM6Ly9zaGVyaWRhbmMtbXkuc2hhcmVwb2ludC5jb20vOnY6L2cvcGVyc29uYWwvcGF0ZWw3OTVfc2hlcm5ldF9zaGVyaWRhbmNvbGxlZ2VfY2EvRVJwU1R1aGlvNUJLczQ0djFhN25kem9CRzlocHpsZTREMjN2UmZCX183WHllQT9ydGltZT1SWXlFVkpfbDEwZw

 

Navneet’s Video Link: https://sheridanc-my.sharepoint.com/personal/sing5831_shernet_sheridancollege_ca/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsing5831%5Fshernet%5Fsheridancollege%5Fca%2FDocuments%2FCognitiveComputing%5FSeminar%5FNavneetSingh%2FCognitiveSeminar%5FNavneetSingh%2Emkv&parent=%2Fpersonal%2Fsing5831%5Fshernet%5Fsheridancollege%5Fca%2FDocuments%2FCognitiveComputing%5FSeminar%5FNavneetSingh&originalPath=aHR0cHM6Ly9zaGVyaWRhbmMtbXkuc2hhcmVwb2ludC5jb20vOnY6L2cvcGVyc29uYWwvc2luZzU4MzFfc2hlcm5ldF9zaGVyaWRhbmNvbGxlZ2VfY2EvRVEwTFRDb3BsVXRHbC1IZUhhdVFRRW9CRktqOU9BYTZSOF9ZSm1TZFQ0NGFLUT9ydGltZT1RVkprWkpfbDEwZw

Deep Patel
1.	Identify the cognitive computing area of interest to be researched and the motivation for your interest.
I (Deep Patel) have decided to research on object identification on Core ML by Apple. I choose to research on object identification since I am planning to use Core ML in my capstone project. Currently, I am using basic Core ML object identification model for product classification in my capstone project. This was my main reason or motivation to research more on Core ML as it will provide me with an opportunity to get in depth understanding of object identification. Another reason for researching on Core ML is that Apple offers brilliant artificial intelligence features such as Vision, Natural Language Processing, Speech and Sound Analysis. 
I am very interested to know how Core ML will classify objects and how perfect the accuracy will be for the classification. Apple have developed very powerful machine learning algorithms by using the Create ML component of Core ML. Core ML is also quite useful doing the facial recognition. However, the main moto of my research is to identify the Apple’s accuracy in classifying the images compared to other famous algorithms available. This Core ML features is very useful framework to implement machine learning algorithms with very few lines of code. Core ML have been developed so far after their first launch and is now available as 3rd version (Core ML 3). 
2.	Describe the problem you are attempting to solve or question you are attempting to answer using the cognitive service and platform identified.
I have to do image classification for my capstone project. Currently, Best Buy have thousands of products at their stores with lot of people visiting the store every single day. Lot of people needs help in identifying the product’s specification. So, they need to wait until an employee comes to assist them and explain about the product. Our team in capstone decided to solve this problem by building an image recognition model which can classify the products by taking an image of the product. We have started to train some images using the Core ML model. However, Best Buy have thousands of products. The problem here is I am not sure about the accuracy of the Core ML model. I am very curious to see how Core ML will train so many images and the processing time for it. For now, we have kept the images down to 10 for each product so that the model can train it possible. However, we are trying to implement different features such as augmentation, cropping, fitting and rotating, etc.  
I believe various features of Core ML such as Create ML and Augmentation will help to solve the above problem and make the life of people easier by just scanning the image and getting all the details. For now, we have achieved very nice accuracy rate for sample models which we have implemented for our capstone project.
3.	Provide an overview of the platform you have researched, the services it provides and resources available, resources that you have used in your research.
Core ML Overview
Core ML is a framework developed by Apple to integrate different machine learning algorithms or models into our application. It provides a layer for integrated representation of different models. Application will use Core ML APIs and various sample user data to make the prediction of the images. It will then use this prediction to train the models. All of this activity will be done on user’s device. 
 
                                                            Figure 1: Core ML Overview
Core ML will help us to apply our own machine learning algorithm depending on the set of training data images which we provide. A testing model will be generated after processing this algorithm. This model will help us to create various new predictions depending on new user inputs. This Core ML models help us to achieve a vast quality of tasks which would be very hard to achieve in other software. Core ML models can actually train the model to classify the images or detecting various objects directly from its pixels.
We can actually build a model and also train it using the default Create ML app which is bundled with Xcode. All the models which are trained using Create ML are in the form of Core ML format which are then available in the application itself. Another way is to actually use different available machine learning libraries and then using Core ML tools to convert in the format of Core ML. We can use the Core ML using user’s data once the model is actually in user’s device. 
Core ML reduces its memory footprint and energy consumption which helps to enhance on-device performance by leveraging CPU, GPU and Neural Engine. There is no need of any kind of network connection if the model is actually running on user’s device. This will help to keep the user’s data personal and responsive app.
It is consider that Core ML have developed domain-specific frameworks and functionality. Core ML is able to identify and support Natural Language Processing which helps to recognise text in images, Sound Analysis which identifies the sounds from audio, Vision to classify products from images and Speech to convert audio from image text. Core ML is built on top of some lower level primitives like Accelerate and BNNS, with Metal Performance Shaders.
 
                                                             Figure 2: Core ML Layers

Neural Networks
It is very important to know how the neural networks works since Apple’s Core ML frameworks supports neural networks. Recently, neural networks have found a lot of success. A neural model is basically trying to understand and process the human mind with various layers of nodes which are connected to each other. An object or image recognition model consists of 48 layers. It contains various matric multiplication of the nodes which is handled by GPU’s very efficiently.  
 
                                                  Figure 3: Neural Network Layers.
However, neural networks need a vast amount of data for training to consider all kinds of possibilities. The neural networks have played a very important role in Core ML’s success.
Create ML
Create ML is used to train machine learning algorithm models on Mac. It can be used to identify and classify images, extract definitions from text and analyzing relationships between various numerical values. 
 
                                                      Figure 4: Create ML.

We have to train the model using Create ML by providing some kind of samples which shows patterns. Models can then be tested by providing some unused images or data to test it. Evaluation can then be done to find out the accuracy of the model. If the accuracy is good enough, model can be integrated with the Core ML application.
                            
                                               Figure 5: Create ML Workflow
Create ML helps to leverage various machine learning algorithms in Apple products such as photos and Siri. This corresponds to smaller natural language and object classification models consuming less time.
Services and Resources
Turi Create: Various own conventional machine learning algorithm models can be built using Turi Create. It is an open-sourced Python library. It helps and supports to train the models on macOS and Linux. It consists of various different tasks for Style Transfer, One-Shot Object Detection and Drawing Classification.
Get Turi Create
IBM Watson Services: Watson Services are very helpful for Core ML in building the applications which can access various Watson’s features directly from iPhone or iPod. This will provide the flexibility to do dynamic insights which can develop as time progress. We can easily click on Watson Services for Core ML because of the IBM Cloud Developer Console for Apple on IBM Cloud. We can even build applications which uses Watson models without network connection. The application will be able to classify images very accurately and able to distinguish visual content. Training will also be very easier using Watson services. 
 
                                                 Figure 6: Core ML using Watson Services
Watson Visual Recognition is so powerful that it can accurately analyze visual content. It has pre-trained models which will be used to classify images for different categories such as food, colors, objects, scenes, and many other content.
Core ML Tools: A Python Package is available to convert the models into Core ML.
Get Core ML Tools
Apache MXNET: This is basically an apache converter to convert the trained machine learning models to the Core ML format. 
Get MXNet model converter
TenserFlow: This is also a converter to convert machine learning models to Core ML format.
Get TensorFlow model converter
ONNX: This is a converter to convert ONNX models to Core ML format.
Get ONNX model converter 
4.	Reflect on the capability of the platform to deliver the cognitive service you have researched
Core ML is a solid platform to perform image or object classification in iOS devices. The Core ML service is very fast and reliable. Core ML has effective way to train and retrieve the results. The image below clearly states the result of a MacBook pro laptop giving 100% accuracy performed for testing. I have provided around 100 images of different products as a training data. I have a 94% accuracy followed by 70% validation on those training data set. I provided an image of MacBook pro for testing from side angle. The result was very awesome with 100% accuracy.
 
Figure 7: MacBook pro

Another test was done with ASUS Zen book laptop. The result was again quite awesome with 100% accuracy identifying it as ASUS Zen book.
 
Figure 8: ASUS Zen book
Strengths:
- Very easy to add Core ML models into iOS application using XCode.
- Many different types of images are easily recognized using Core ML Model.
Limitations:
- Biggest limitation of Core ML is that it only works with iOS devices.
- Even in iOS devices, it only supports iOS 11 and later only.
5.	Describe how does your results compare with the results of your seminar partner. If the same service was researched on separate platforms compare the platforms. If different services were researched, compare the capabilities of the platform to deliver those services in comparison to the IBM Platform.
My seminar partner (Rathin Chopra) have researched on Google Vision. So, I will be comparing Core ML with Google Vision. As mentioned earlier in weaknesses, Core ML only supports iOS devices. It is not a cross-platform suite. It doesn’t support Android or google devices. Google is very rich in providing default machine learning models and APIs. However, Apple’s Core ML is little behind in that context. Apple still do have a better Vision API framework which actually helps to build applications with on-device face detection, text recognition and barcode scanning.
The biggest names that comes into mind when vision is considered are Microsoft Azure, Google Vision, Amazon Web Services (AWS) and IBM Cloud. Core ML have done a great job in maintaining their standard against such powerful companies and established the market. However, I still believe Google Vision is better than Core ML as it has the biggest dataset. Also, it can be accessed by all devices. Core ML models only work in iOS devices which make it less convenient for other platforms to use it. 
Core ML is not completely developed. It is still in progress. All the developers who uses Google’s firebase are more likely to prefer Google’s ML Kit rather than Core ML.
6.	Create a walkthrough tutorial like the ones we have used to learn how to use Watson or Slack to develop conversational apps
Creating an image classification model: 
1. Opening CreateML in MacBook pro.
 
                                                            Figure 9: Open CreateML





2. Choose a template. For now, select image classifier
 
                                                           Figure 10: Choose Template
3. Name the model. (I gave MyFirstImageClassifier)
 
                                                         Figure 11: Name the Model
4. Choose the training data set images
 
Figure 12: Choosing the training dataset images
5. Dataset should have multiple products with labels and images in it.
 
Figure 13: Dataset

6. Once the dataset is selected, click on Train.
 
Figure 14: Training Dataset


7. Screen after training is completed.
 
Figure 15: Training Completed

8. Adding Testing Data Images
 
Figure 16: Adding Testing Data Images

9. Result of Testing Data with its confidence
 
Figure 17: Result of Testing Data


Rathin Chopra
1.	Identify the cognitive computing area of interest to be researched and the motivation for your interest.
I (Rathin Chopra) chose to research on object identification on Cloud AI by Google. The reason/motivation behind choosing Google AI platform is that I want to know how a giant company does who is leading in many other platforms is performing in terms of AI. The main reason was that I read that google offers many different AI services such as sight also known as Vision AI, Natural Language, Language Translation, Text to speech, speech to text, structured data, and Cloud AutoML. As in my capstone project, I am using the vision services provided by Apple (CoreML) and I wanted to look at other options available in the market and make sure that me and my team has chosen the right platform for Vision AI.
I do genuinely want to know how Cloud AI by Google will perform in terms of accuracy as they will have the biggest database for images and many other information-like videos, text and etc. The most interesting thing that made me choose google AI is that Google provides two ways to obtain insight into the abundance of information hidden in your pictures. Google’s efficient pre-trained Vision API models instantly categorise pictures into hundreds of categories (such as "sailboat" or "Eiffel Tower"), and identify individual things, expressions, and words. Google does use the thousands of images that are uploaded to google database every second to get the labels and stuff. It also allows to train your own model with your own images.
2.	Describe the problem you are attempting to solve or question you are attempting to answer using the cognitive service and platform identified.
I am trying to learn about the vision services that google provides so that I can compare them with the coreML by apple. The problem that I am trying to solve is to train a model with more than 500,000 images of electronic product and more than 200,000 products. As this problem is related to my capstone project, I am curious how google actually handles all these images and which neural network do they use to classify and train the model. Right now, my team are in deciding phase on which platform to go on with or should we build a model in python itself. 
My problem is basically how google will perform in differentiating many similar products like laptops or like TV’s. In my capstone project, we have this issue that we are trying to resolve, and this issue can be really difficult to tackle as many laptops can have similar features and the image set for one product is limited to 4-6 images as well.
I would also like to know more about the AutoML Vision that google provides. According to google, AutoML Vision enables you to train machine learning models to classify your images according to your own defined labels. They guarantee high accuracy score as well, so I would like to test that or will try to use it and see the accuracy.
3.	Provide an overview of the platform you have researched, the services it provides and resources available, resources that you have used in your research.
The following summary/overview is intended to help you grasp the overall image of Google Cloud Platform. Google Cloud Platform comprises of a collection of assets, such as servers and storage drives, as well as virtual infrastructure, such as virtual machines (VMs), located in Google's global data centres. These resources are distributed over bunch of regions in different continents. Google have invested a lot of time and money in many services like Machine Learning, cloud storage, cloud computing. Google cloud platform a lot of services but for now I will list some of the major ones. The major services provided by Google cloud are Computing and hosting, Storage and cloud storage, Databases, Networking, Big Data, (AI and Machine Learning), and Data Analytics. 
Computing and hosting services
Google Cloud provides many options for computing and hosting. The options are listed below:
a.	Work in a serverless environment: Google’s cloud functions as a service provides a serverless execution environment for building and connecting cloud services. With the help of these FaaS the client can literally just write a simple function that is attached to the events triggered by the cloud services. These cloud functions are written using JavaScript and can be executed in Node.js environment. Due to the reasons that the Cloud Function can run in any standard Node.js runtime and that makes portability and testing easy.
These Cloud Functions by Google are really a good option for use cases that include the following:
•	Data processing and ETL operations, for scenarios such as video transcoding and IoT streaming data.
•	Webhooks to respond to HTTP triggers.
•	Lightweight APIs that compose loosely coupled logic into applications.
•	Mobile backend functions.
b.	Use a managed application platform: App Engine is Google Cloud's platform as a service (PaaS). With this app engine/platform, google will take care of most of the management of the major resources for the client. When you build your app on App Engine, you can:
•	Build your app in Go, Java, .NET, Node.js, PHP, Python, or Ruby and use pre-configured runtimes, or use custom runtimes to write code in any language.
•	Let Google manage app hosting, scaling, monitoring, and infrastructure for you.
c.	Leverage container technologies to gain lots of flexibility: These containers allow the user/client to focus on their application code, instead of the deployments and integration into the hosting environments. Google Kubernetes Engine (GKE), the Google Cloud Containers as a Service (CaaS) service, is based on the Kubernetes open source architecture, which provides you with on-site or hybrid cloud functionality, in addition to the centralised cloud infrastructure of Google Cloud.
When you build with GKE, you can:
Create and manage groups of Compute Engine instances running Kubernetes, called clusters. GKE uses Compute Engine instances as nodes in a cluster. Each node runs the Docker runtime, a Kubernetes node agent that monitors the health of the node, and a simple network proxy.
Declare the requirements for your Docker containers by creating a simple JSON configuration file.
d.	Build your own cloud-based infrastructure to have the most control and flexibility (Virtual Machines): These Compute engines can be considered as infrastructure as a service (IaaS). These systems provide a robust computing infrastructure. It is Google’s responsibility to make sure that all the resources are available, reliable and ready for the virtual machine. While building a virtual machine in Google’s cloud, Google provides the following features:
•	Create instances from public or private images.
•	Use GCP storage technologies or any third-party technologies you prefer.
•	Use Google Cloud Marketplace to quickly deploy pre-configured software packages. For example, you can deploy a LAMP or MEAN stack with just a few clicks.
Storage services
Whatever your application whether is a big company application like Banks or whether it is just a small game, the application will require some kind of storage. The application has to store the media files, backups or any other file-like objects and Google does provide a wide range of storage services like: Cloud storage, and Compute Engine provides both hard-disk-based persistent disks, called standard persistent disks, and solid-state persistent disks.
Database services
Google Cloud has a variety of SQL and NoSQL database services. A SQL database in Cloud SQL, which provides either MySQL or PostgreSQL databases. There are two options for NoSQL data storage: Firestore, for document-like data, and Cloud Bigtable, for tabular data.
Networking services
Google provides Virtual Private Cloud also known as VPC. Basically, VPC delivers a bunch of networking services that VM instances might need. An instance is allowed to have more than one interface, but every interface should be connected to a different network. In this networking services, Google has added two types of load balancing. Network load balancing provides the client to distribute traffic amongst many server instances in the same region based on incoming IP protocol data, such as address, and port. HTTP(S) load balancing allows user to spread traffic throughout regions to guarantee that the requests are routed to the nearby region or, in the event of a failure, to a working instance in the next nearby region.
Big data services
Big data services allow to process and query big data in the cloud to get quick answers to complex problems. BigQuery also offers data analysis services. Following are the services provided by BigQuery:
•	Create custom schemas that organize your data into datasets and tables.
•	Load data from a variety of sources, including streaming data.


AI and Machine learning services
Google leading AI Platform have a diversity of powerful machine learning (ML) services. Google Cloud have direct APIs that allows to access Google's ML without the hassle of creating and training your own models, but if a client wishes to train your personal model, they can do that too.
•	Video Intelligence API allows to use video analysis expertise that delivers label detection, explicit content detection and many other.
•	Speech-to-Text allows to change audio to text and can recognize over 110 languages and variants as well.
•	Cloud Vision allows to effortlessly integrate vision detection features, that includes image labeling, face and landmark detection, optical character recognition and many more.
•	Cloud Natural Language API has some cool features like sentiment analysis, entity analysis, entity-sentiment analysis, content classification, and syntax analysis.
Google’s AI Platform take the advantage of both managed infrastructure of GCP with the power of TensorFlow. This allows to train your machine learning models at a big scale, and to host trained models to make predictions about new data.
I have researched on Google’s Vision AI and cloud AutoML. Cloud AutoML allows to train custom machine learning model whether it is for image classification or object identification. In a nutshell, it is a set of machine learning products that permits developers with partial machine learning expertise to train high-quality models.
4.	Reflect on the capability of the platform to deliver the cognitive service you have researched
According to me, Google Cloud Platform has built a pretty solid all-in-one platform. From a client’s perspective, you can get every service that you will need by Google itself and they are using state-of-the-art transfer learning and neural architecture search technology that allows them to have really nice performance and also allows them to make models on high quality training sets. So, I do feel that Google’s Vision API and AutoML Vision for image classification can produce really nice results depending upon how you train your model. 
(In reference to the image below) The Google vision service without training was able to get the proper result with the exact company name. Google uses a clever technique that tells when this image is shown as the result on google search engine.
 
Figure 18: Result of Vision API (Web)
It is crazy that it recognized the place and the steps and gave me the location of the place where this image was taken. That is really impressive.
 
Figure 19: Result of Vision API (Landmarks)
 
Figure 20: Result of Vision API (Labels)
Provider	Strengths	Weakness
Google	•	Designed for cloud-native businesses
•	Commitment to open source and portability
•	Deep discounts and flexible contracts
•	DevOps expertise	•	Late entrant to IaaS market
•	Fewer features and services as compared to Microsoft.
•	Not as enterprise focused as the other providers.

5.	Describe how does your results compare with the results of your seminar partner. If the same service was researched on separate platforms compare the platforms. If different services were researched, compare the capabilities of the platform to deliver those services in comparison to the IBM Platform.
Google performs way better than the other platforms in many ways. Google jumped on this AI/cloud bandwagon in 2008 and google has done major increments in this AI/cloud world. Obviously, the major players in this world are Microsoft Azure, Google Cloud Platform (GCP), Amazon Web Services (AWS), and IBM Cloud. While GCP is lagging behind AWS with respect to market share, it is trying to build its market presence and it is clearly seen by that Google was able to score Apple and Spotify from Amazon's AWS. Following are the reasons that Google is better than the other platforms:
•	Incorporating Google searches and Maps in Cloud AutoML Vision: This feature gives google the biggest advantage over the competition (Microsoft and Apple). Google vision is able to identify the exact location of image or if there is any recognizable landscape in the image. As I have shown in the above examples (Figure 1), Google also cross validated the image recognition answer with Google’s search engines searches and as everyone knows they are the biggest and best search engine available.
•	Big on AI: AI and deep learning are major focus areas for Google Cloud Platform. Thanks to TensorFlow, an open source software library developed to create machine learning applications, Google is a pioneer in AI development due to Tensorflow. Comparison of the main AI services provided by Google and Microsoft is shown below.
 
Figure 21: Differences of Microsoft Azure and Google cloud
•	Pricing: Why most of the big companies are shifting towards Google cloud platform is because of their reasonable pricing. One appealing aspect of pricing provided by Google is the Dedicated Usage Discounts. In this programme, if you pledge consumption for either 1 or 3 years, you can buy "a particular quantity of vCPUs and memory for up to 57 per cent off normal rates." An additional bonus is that all virtual machines in operation are used. On the other Microsoft and IBM uses some complex plans that makes it harder to recommend.
•	Speed: From what I have researched, Google is leading in the department of speed. Google has invested a ton of money in faster cables so, that they can increase the speed of these services flowing through cloud to 10Tb/s. Due to these high speed, Google is able to process more data in way less time and also reduces the price of networking as they have their own private network cables. Henceforth, Google’s huge physical network is the main reason that it gets an advantage over other cloud providers. Google also assures low-latency network infrastructure. These kind off speed is not provided by any other Cloud service provider like Apple or Microsoft.
•	Live Migration: Google provides live migration of virtual machines from host machines as opposed to other platforms like Microsoft and Apple, enabling enterprises to be up and running 24 * 7 without any efficiency hindrances. Google have implemented straightforward servicing, which incorporates data and software as compared to other providers. Live Migration helps the clients to restore and upgrade applications including security related programmes without rebooting the equipment.
•	Big Data: As we know that Amazon Web Services is better known for its computing and infrastructure, Microsoft Azure for its operating system, Google sticks out for its data analytics. Google’s big data technology advances like MapReduce, Bigtable and the revolutionary services and frameworks for cloud data warehousing (BigQuery), advanced machine learning (AI Platform), and stunning visual analytics (Google Data Studio) provides powerful data insights. Google Cloud big data analytics approaches are serverless, that removes the difficulty of making and upholding a data analytics system and that allows to accelerate the time-to-insight.
•	Google-grade security: Google's security model is an end-to-end operation, based on more than 15 years of professional experience focusing on keeping consumers secure on Mobile apps like Gmail and Google Maps. With Google cloud the applications take advantage of the same protection architecture.
6.	Create a walkthrough tutorial like the ones we have used to learn how to use Watson or Slack to develop conversational apps
Setting your project:
1.	Inside Cloud Console, on the project selector page, select create a new Cloud project.
2.	Make sure to enable the AutoML and Cloud Storage APIs.
3.	Install the gcloud command line tool.
4.	Change the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path to the service account key file that you downloaded while creating the service account.
export GOOGLE_APPLICATION_CREDENTIALS=key-file
5.	Set the PROJECT_ID environment variable to your new Project ID.
export PROJECT_ID=your-project-id
6.	Add your service account to AutoML account
gcloud auth login
gcloud projects add-iam-policy-binding $PROJECT_ID \
   --member="serviceAccount:service-account-name" \
   --role="roles/automl.editor"
Preparing your training Data
According to Google, the training data must be as close as achievable to the data on which predictions are to be made. Google advocate about 1000 training images for each label. The minimum images per label is 10, or 50 for complex models.
1.	Upload all your training images to Google cloud storage.
2.	Now create a CSV file that lists all of your training data and the category labels for that data.
3.	Make sure the content is categorized and labelled in the CSV file, a comma-separated list of labels that identify how the image is categorized. For example:
Labeled: gs://my-storage-bucket-vcm/flowers/images/img100.jpg,daisy
Creating a dataset
The very first step in making a custom model is to make an empty dataset that will later on will be holding your training data.
1.	Open the Vision Dashboard
 
Figure 22: Dashboard
2.	Select the Datasets from the left side of navigation menu.
3.	Click on the New Dataset button at the top, and then select the radio_button_checked single-label or multi-label classification based on the data.
 
Figure 23: create a new database
4.	When you are done specifying the class type, click Create Dataset.
5.	Create Dataset page will allow you to choose a CSV file from Google Cloud Storage, or local image files that you can import into the dataset.
 
 
Figure 24: Select files to import
6.	Click on continue to start image importing into your dataset. You should get an email when the importing is done.
Importing items into a dataset
1.	Open the Vision Dashboard and click on the dataset from the Datasets page.
 
Figure 25: Datasets
2.	Now you will land on Images page, select Add items in the title bar of the page and click on the import method from the drop-down list.
3.	Select the file or files that you would like to import.
Labeling training items
1.	If you want to label entries in the AutoML Vision UI, click on the dataset from the Datasets page to see its details.
2.	On the side bar you will be able to filter the item list by label or click on Add new label to create a new label.
 
Figure 26: Dataset dashboard
Training models
1.	Open the Vision Dashboard
2.	Click on the dataset that you would like to use to train the custom model.
 
Figure 27: click train button
3.	Once you are done reviewing the model, click on the train button as shown in the image above.
4.	Once the dataset is ready, click on Start Training.
5.	Provide the model name and also make sure to select the cloud hosted button.
 
Figure 28: Define your model
6.	Choose appropriate training budget value based on your dataset size, in the Set a node hour budget section. 
7.	Click on start training.

Deploying your model
1.	Go to the Test & Use tab below the title bar.
2.	Choose the Deploy model button.
 
Figure 29: Test & use
3.	Enter the number of nodes you want to deploy the model with.
 
Figure 30: Deploy button
4.	Click on the deploy button, to start the deployment, and once the model is deployed you will get an email.








Navneet Singh
1. Identify the cognitive computing area of interest to be researched and the motivation for your interest.
The cognitive area of research chosen here is Cognitive Vision Services provided by Microsoft and in-depth analysis of image to description service. The motivation behind researching this cognitive service aspect came from the capstone project I’m working on, that is to scan any government issued identification document such as driving license or health card to a description of information fetched from the document in the form of suitable text fields such as name, address and expiry date. 
In order to choose the Microsoft platform, the vision services provided by Microsoft appealed me to research in this field in order to see what features I can extract from these services. There’s a need to look for a secured vision service in order to prevent the sensitive data from driving license or any other ID, that’s why the services provided by Microsoft were chosen as they claim to provide more secure platform. So, while looking at the requirements of image to description service on a government issued ID, this research is being conducted.
2. Describe the problem you are attempting to solve or question you are attempting to answer using the cognitive service and platform identified.
The problem came up while brainstorming about validating the user for the mobile voting process, there was a need to look up for some digital solution on how to verify and validate the user in order to give them an access to vote on their mobile device. The normal paper ballot voting goes with physical appearance of an individual with their proof of residence. In order to bring this process for online voting, the question came up on how to verify and validate the identification of the user. In order to verify the user for mobile voting process the idea of an image to text description from government issued ID, primarily driving license came up. The goal is to retain some fields from the driving license image uploaded in the application while the voter is getting on board to vote online. The uploaded driving license image can extract name, address and expiry date from the image and could verify that data with election board’s voter list data in order to validate the user, so that they could be eligible for voting from their mobile devices after validating themselves. Using Microsoft Azure’s Computer Vision service, there’s a possibility to solve this problem, as this service provide an access to advanced image processing algorithms which return the required information. 
3. Provide an overview of the platform you have researched, the services it provides and resources available, resources that you have used in your research.
Microsoft is one of the major cloud service providers of AI services and cognitive services signifies on of the largest sets of AI capabilities which includes deep learning, sentiment evaluation and machine vision. Azure Cognitive Services provide and ease to the developers by providing them APIs and SDKs to easily add the cognitive features to their applications. Azure Cognitive Services provide services inf five major domains – Vison, Speech, Language, Web Search and Decision. Azure Cognitive services provide enough resources to the developers such that they can use these services efficiently by having them on various software development kits such as C#, Python, Java and much more. It also has additional resources such as Azure CLI, PowerShell and RESTful APIs for cognitive Service Management. While looking at the problem and the motivation of the project, this research will focus on services provided by Azure in Vision in order to recognize, identify, caption and moderate the pictures through the cognitive services provided under this category. An overview of Language aspect will be considered in order to study Text Analytics and Immersive Reader service provided under Language category which allows the apps to process natural language with pre-built scripts to recognize the user demands. 
Azure Vision API with Services:
•	Computer Vision – Provides the access to advanced algorithms for processing images and returning information.
•	Custom Vision Service – Allows the authority to build custom image classifiers.
•	Face – Provides access to advanced face algorithms that allows face detection attributes and recognition.
•	Form Recognizer – Identifies and extracts data from the documents and outputs more organized and structured data including its relationship from the original content.
•	Ink Recognizer – Enables the recognition and analyzation of digital ink data, shapes and handwritten content, and outputs a structured document with all the related entities.
While looking at the problem of implementing image to text service on driving license, there’s a need to have an in-sight analysis of Azure’s Custom Vision Cognitive Service. It let the developers build, deploy and improve their own image classifiers that is an AI service that applies labels to the images as per their graphic appearances. This service uses a machine learning algorithm to apply labels to the images and the images must be taken from different types in this case license category such as G1, G2 and G (for Ontario Driving Licenses). Then the algorithm trains itself to the data provided and calculates the test accuracy. Hence, this is further expanded to image classification and object detection for providing more details of model image. This service provides an opportunity to create, test and train a customized model on their native SDK as well as through a web interface on Azure Custom Vision. The major aspect of using this platform is Data Privacy and Security which it very well satisfies. For uploading the image while using Vision service, none of the data is being stored or given to unauthorized persons as there’s an automatic deletion of data after the processing is being done which means the details fetched through image-to text description on driving license will be protected and not saved over the cloud service. 
4. Reflect on the capability of the platform to deliver the cognitive service you have researched.
Azure can provide several Cognitive Vision actions to analyze the image such as object detection, image categorization, image description, face detection, image types, detect colors and much more through its advanced algorithms. The major problem solver is the extraction of text from images through Computer Vision Read API to extract text from images. The examples shown below provides the image description of tags and model trained by Computer Vision service. In order to extract the name and address fields from the image, there’s a need to define a custom model through Custom Vision Service offered by Azure to fetch the more detailed and required results. One of the major strength is that the response time is very quick, and the options to upload the image is through is browsing it from local device or the URL It is tightly integrated with Microsoft Azure, SQL database and virtual machine as an end to end solution. Some of the usage based pricing model can be difficult to comprehend and implement effective cost controls.
 
Figure 31: Test Image through Computer Vision Service
 
Figure 32: Test Image through Computer Vision Service
5. Describe how does your results compare with the results of your seminar partner. If the same service was researched on separate platforms compare the platforms.
Computer Vision API provided by Microsoft Azure gives access to advanced image processing algorithms returning information after analyzing the images. By uploading an image, Microsoft Computer Vision algorithms can analyze visual content in different ways based on inputs and user choices. Microsoft Azure Machine Leaning Studio provides the following automated ML services such as classification, regression, clustering and anomaly detection. The clustering (K-means) algorithm provides a great graphical interface for several methods including detection, regression, recommendation and text analysis. It is available on different platforms that includes python packages, model management and Visual Studio Tools for AI. Microsoft’s Speech and Text API has services like spell check, auto completion, voice verification which are not provided other google could platform. Among the image analysis API comparison, both Microsoft and Google have object detection services, facial analysis, text recognition whereas google doesn’t have face recognition service.
Google Cloud Auto ML is a cloud-based machine learning platform for uploading datasets, train model and deployment. Cloud Vision API gives better understanding of the content and hidden information of an image by using machine learning models with the help of REST API. It classifies images into thousands of categories, detects unique objects and faces within the images, and reads printed words that are contained within images. This API gives access to build metadata on the image catalog, moderate offensive content and enables the use of new marketing scenarios through image sentiment analysis. In comparison to automated ML services it doesn’t provide clustering and anomaly detection services.
6. Create a walkthrough tutorial like the ones we have used to learn how to use Watson or Slack to develop conversational apps.
Test and Retrain a model with Custom Vision Service
Step 1 – Test your model
1.	Select the project from Custom Vision web page and select quick test from top right.
 
Figure 33: Quick Test


2.	In Quick Test window, select the URL or browse local files to upload the image for test.
 
Figure 34: Select Image
The results of the selected image are shown in the form of the table with tags and confidence. Further this image could be tested to the model and then retrain it.
3.	Now use the predicted image for training. In order to view images submitted to the classifier, click on Prediction tab on Custom Vision web page.
 
Figure 35: Prediction Tab
4.	To add the tags to the training data, select the image and then select the tag, and then click save and continue to retrain the classifier.
  
Figure 36: Add tags
Step 2 – Improve your classifier
1.	Add more images and balance the data by retraining.
2.	Add images with varying background, lighting, object size, camera angle and style, and then again retrain.
 
Figure 37: Adding images
3.	Use the new images to test prediction.
4.	Modify the existing training data according to prediction results.
 
Figure 38: Modify existing training data

Step 3 – Use the prediction API
1.	From Custom Vision web page, select the project and then Performance tab. To submit the images to the Prediction API, select Publish for the specified iteration. This will make the model accessible to Prediction API of Custom Vision Azure resource. 
 
Figure 39: Publish Prediction API

2.	Once the model has been published, get the Prediction URL and Prediction Key
 
Figure 40: Get Prediction URL

 
Figure 41: How to use Prediction API

Step 4 – Create and Run the Application











References
Training Cloud-hosted models | Cloud AutoML Vision | Google Cloud. (n.d.). Retrieved April Training Cloud-hosted models | Cloud AutoML Vision | Google Cloud. (n.d.). Retrieved April 20, 2020, from https://cloud.google.com/vision/automl/docs/train 
Cloud AutoML - Custom Machine Learning Models | Google Cloud. (n.d.). Retrieved April 20, 2020, from https://cloud.google.com/automl 
Core ML. (n.d.). Retrieved April 20, 2020, from https://developer.apple.com/documentation/coreml 
Introduction to AI Platform | Google Cloud. (n.d.). Retrieved April 20, 2020, from https://cloud.google.com/ai-platform/docs/technical-overview 
n.d. alexsoft. https://www.altexsoft.com/blog/datascience/comparing-machine-learning-as-a-service-amazon-microsoft-azure-google-cloud-ai-ibm-watson/. 
n.d. Custom Vision. https://azure.microsoft.com/en-ca/services/cognitive-services/custom-vision-service/.
n.d. Microsof Azure. https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home.
Ng, Jed. n.d. Medium. https://medium.com/rakuten-rapidapi/top-10-computer-vision-apis-aws-microsoft-google-and-more-fe6fe9a9bc8c.
n.d. Test and Retrain a model with Custom Vision Service. https://docs.microsoft.com/en us/azure/cognitive-services/custom-vision-service/test-your-model. 

